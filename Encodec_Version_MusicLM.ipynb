{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MusicLM - using LucidRain's pytorch implementation. "
      ],
      "metadata": {
        "id": "tUf4Y-2zkrxw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO9HMDnMje4v"
      },
      "outputs": [],
      "source": [
        "!pip install musiclm-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install audiolm-pytorch this mgiht be needed idk"
      ],
      "metadata": {
        "id": "VDUnV8JH1POb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install encodec"
      ],
      "metadata": {
        "id": "8NXddgjasQQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oOjgPAeEsRxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from musiclm_pytorch import MuLaN, AudioSpectrogramTransformer, TextTransformer\n",
        "from encodec import EncodecModel\n",
        "from encodec.utils import convert_audio\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch\n",
        "from IPython.display import Audio\n",
        "import torchvision.transforms as transforms  \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define all dataset paths, checkpoints, etc\n",
        "dataset_folder = \"placeholder_dataset\"\n",
        "soundstream_ckpt = \"results/soundstream.8.pt\" # this can change depending on number of steps\n",
        "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
        "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
      ],
      "metadata": {
        "id": "O79eaD2pj_nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sinewave(freq=440.0, duration_ms=200, volume=1.0, sample_rate=44100.0):\n",
        "  # code adapted from https://stackoverflow.com/a/33913403\n",
        "  audio = []\n",
        "  num_samples = duration_ms * (sample_rate / 1000.0)\n",
        "  for x in range(int(num_samples)):\n",
        "    audio.append(volume * math.sin(2 * math.pi * freq * (x / sample_rate)))\n",
        "  return audio"
      ],
      "metadata": {
        "id": "GHj3KmunpVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder data generation\n",
        "def save_wav(file_name, audio, sample_rate=44100.0):\n",
        "  # Open up a wav file\n",
        "  wav_file=wave.open(file_name,\"w\")\n",
        "  # wav params\n",
        "  nchannels = 1\n",
        "  sampwidth = 2\n",
        "  # 44100 is the industry standard sample rate - CD quality.  If you need to\n",
        "  # save on file size you can adjust it downwards. The stanard for low quality\n",
        "  # is 8000 or 8kHz.\n",
        "  nframes = len(audio)\n",
        "  comptype = \"NONE\"\n",
        "  compname = \"not compressed\"\n",
        "  wav_file.setparams((nchannels, sampwidth, sample_rate, nframes, comptype, compname))\n",
        "  # WAV files here are using short, 16 bit, signed integers for the \n",
        "  # sample size.  So we multiply the floating point data we have by 32767, the\n",
        "  # maximum value for a short integer.  NOTE: It is theortically possible to\n",
        "  # use the floating point -1.0 to 1.0 data directly in a WAV file but not\n",
        "  # obvious how to do that using the wave module in python.\n",
        "  for sample in audio:\n",
        "      wav_file.writeframes(struct.pack('h', int( sample * 32767.0 ))) # Sort this out, this is needless\n",
        "  wav_file.close()\n",
        "  return\n",
        "\n",
        "def make_placeholder_dataset():\n",
        "  # Make a placeholder dataset with a few .wav files that you can \"train\" on, just to verify things work e2e\n",
        "  if os.path.isdir(dataset_folder):\n",
        "    return\n",
        "  os.makedirs(dataset_folder)\n",
        "  save_wav(f\"{dataset_folder}/example.wav\", get_sinewave())\n",
        "  save_wav(f\"{dataset_folder}/example2.wav\", get_sinewave(duration_ms=500))\n",
        "  os.makedirs(f\"{dataset_folder}/subdirectory\")\n",
        "  save_wav(f\"{dataset_folder}/subdirectory/example.wav\", get_sinewave(freq=330.0)) # take this line out and sine wave thing for real training attempt\n",
        "\n",
        "make_placeholder_dataset()"
      ],
      "metadata": {
        "id": "dbJ4xN4eo_-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train MuLan:\n",
        "\n",
        "This part isn't finished, need to replace wavs and then I think it's done. "
      ],
      "metadata": {
        "id": "38UqJYg1qxRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_transformer = AudioSpectrogramTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64,\n",
        "    spec_n_fft = 128,\n",
        "    spec_win_length = 24,\n",
        "    spec_aug_stretch_factor = 0.8\n",
        ")\n",
        "\n",
        "text_transformer = TextTransformer(\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    dim_head = 64\n",
        ")\n",
        "\n",
        "mulan = MuLaN(\n",
        "    audio_transformer = audio_transformer,\n",
        "    text_transformer = text_transformer\n",
        ")\n",
        "\n",
        "# get a ton of <sound, text> pairs and train\n",
        "\n",
        "wavs = torch.randn(2, 1024)\n",
        "texts = torch.randint(0, 20000, (2, 256))\n",
        "\n",
        "loss = mulan(wavs, texts)\n",
        "loss.backward()\n",
        "\n",
        "# after much training, you can embed sounds and text into a joint embedding space\n",
        "# for conditioning the audio LM"
      ],
      "metadata": {
        "id": "G7CYMsj2jhB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeds = mulan.get_audio_latents(wavs)  # during training ?? what does this mean, during training vs inference?\n",
        "\n",
        "embeds = mulan.get_text_latents(texts)  # during inference, I mean do these bits even need to run, this embeds would overwrite the one above"
      ],
      "metadata": {
        "id": "MpKFSkj9z2BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do data loader. \n",
        "\n",
        "But what does this mean? "
      ],
      "metadata": {
        "id": "FsQd9Ohfw1-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_path = dataset_folder"
      ],
      "metadata": {
        "id": "fShNqb0I096n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wavs = torch.randn(2, 1024)\n",
        "texts = torch.randint(0, 20000, (2, 256))"
      ],
      "metadata": {
        "id": "LIXQRVSmj9B8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wavs.shape, texts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl9fNf18kFNk",
        "outputId": "4e5164e0-522b-4288-8735-11e2b25fccad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 1024]), torch.Size([2, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from musiclm_pytorch import MuLaNEmbedQuantizer\n",
        "# setup the quantizer with the namespaced conditioning embeddings, unique per quantizer as well as namespace (per transformer)\n",
        "quantizer = MuLaNEmbedQuantizer(\n",
        "    mulan = mulan,                          # pass in trained mulan from above\n",
        "    conditioning_dims = (1024, 1024, 1024), # say all three transformers have model dimensions of 1024\n",
        "    namespaces = ('semantic', 'coarse', 'fine')\n",
        ")\n",
        "# now say you want the conditioning embeddings for semantic transformer\n",
        "wavs = torch.randn(2, 1024)\n",
        "conds = quantizer(wavs = wavs, namespace = 'semantic') # (2, 8, 1024) - 8 is number of quantizers, dunno what the two is about, stereo audio?"
      ],
      "metadata": {
        "id": "0VEb-wHijlkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Transformer:"
      ],
      "metadata": {
        "id": "G3vW4e1slvDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from audiolm_pytorch import HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer"
      ],
      "metadata": {
        "id": "usWyrQVKp2jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hubert checkpoints can be downloaded at\n",
        "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "if not os.path.isdir(\"hubert\"):\n",
        "  os.makedirs(\"hubert\")\n",
        "if not os.path.isfile(hubert_ckpt):\n",
        "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
        "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
        "if not os.path.isfile(hubert_quantizer):\n",
        "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
        "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
        "\n",
        "# work out if we need to be getting or dl the checkpoints if not already there\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = './hubert/hubert_base_ls960.pt',\n",
        "    kmeans_path = './hubert/hubert_base_ls960_L9_km500.bin'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True      # this must be set to True (same for CoarseTransformer and FineTransformers)\n",
        ").cuda()\n",
        "\n",
        "trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    audio_conditioner = quantizer,   # pass in the MulanEmbedQuantizer instance above\n",
        "    folder = ds_path,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 1000 # idk the number of training steps needed here for the semantic transformer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "5oMWg1OojmyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coarse Transformer:"
      ],
      "metadata": {
        "id": "koxM_PxJl0zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from audiolm_pytorch import CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM"
      ],
      "metadata": {
        "id": "wUCOeXRtyDJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from audiolm_pytorch import EncodecWrapper"
      ],
      "metadata": {
        "id": "jUmihEz2o1tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")\n",
        "\n",
        "# Instantiate a pretrained EnCodec model\n",
        "encodec = EncodecWrapper(\n",
        "        target_sample_hz = 24000,\n",
        "        strides = (2, 4, 5, 8),\n",
        "        num_quantizers = 8,\n",
        ")\n",
        "\n",
        "coarse_transformer = CoarseTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    codebook_size = 1024,\n",
        "    num_coarse_quantizers = 3,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True # adding audio text cond true    \n",
        ")\n",
        "\n",
        "trainer = CoarseTransformerTrainer(\n",
        "    transformer = coarse_transformer,\n",
        "    codec = encodec, # change to encodec\n",
        "    wav2vec = wav2vec,\n",
        "    folder = ds_path,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 1000,\n",
        "    save_model_every = 1000,\n",
        "    num_train_steps = 10000,\n",
        "    \n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "3fztSIXzlogz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine transformer"
      ],
      "metadata": {
        "id": "OyqSw_ucl6nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodec = EncodecWrapper(\n",
        "        target_sample_hz = 24000,\n",
        "        strides = (2, 4, 5, 8),\n",
        "        num_quantizers = 8,\n",
        ")\n",
        "\n",
        "fine_transformer = FineTransformer(\n",
        "    num_coarse_quantizers = 3, # could test 2 by 2 if using n_q = 4, or 1 by 3\n",
        "    num_fine_quantizers = 5,\n",
        "    codebook_size = 1024,\n",
        "    dim = 512,\n",
        "    depth = 6,\n",
        "    audio_text_condition = True   # adding audio text cond true\n",
        ")\n",
        "\n",
        "trainer = FineTransformerTrainer(\n",
        "    transformer = fine_transformer,\n",
        "    codec = encodec, # change to encodec\n",
        "    folder = ds_path,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 10000,\n",
        "    save_model_every = 1000\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "y15XMVNjl6En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference:"
      ],
      "metadata": {
        "id": "IpbYVT4QmBTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodec = EncodecWrapper(\n",
        "        target_sample_hz = 24000,\n",
        "        strides = (2, 4, 5, 8),\n",
        "        num_quantizers = 8,\n",
        ")"
      ],
      "metadata": {
        "id": "zmuXAPaqqFEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Everything together\n",
        "audiolm = AudioLM(\n",
        "    wav2vec = wav2vec,\n",
        "    codec = encodec,\n",
        "    semantic_transformer = semantic_transformer,\n",
        "    coarse_transformer = coarse_transformer,\n",
        "    fine_transformer = fine_transformer\n",
        ")\n",
        "\n",
        "generated_wav = audiolm(batch_size = 1)"
      ],
      "metadata": {
        "id": "R5ro8KRfvMk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you need the trained AudioLM (audio_lm) from above\n",
        "# with the MulanEmbedQuantizer (mulan_embed_quantizer)\n",
        "from musiclm_pytorch import MusicLM\n",
        "\n",
        "musiclm = MusicLM(\n",
        "    audio_lm = audio_lm, # ? do we need (batch_size = 1) here as well?\n",
        "    mulan_embed_quantizer = mulan_embed_quantizer\n",
        ")\n",
        "\n",
        "music = musiclm('the crystalline sounds of the piano in a ballroom', num_samples = 4) # sample 4 and pick the top match with mulan"
      ],
      "metadata": {
        "id": "dLpAgQGejpNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}